{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 이순신은 조선 중기의 무신이다.\n",
      "Encoded: tensor([[    2, 10661,  2073,  3957,  9652,  2079, 15749, 28674,    18,     3]])\n",
      "Tokenized: ['이순신', '##은', '조선', '중기', '##의', '무신', '##이다', '.']\n",
      "\n",
      "Original: 오늘가치 조은 날에는 술 한병 지버들고 뒷사네 오르고 시퍼라.\n",
      "Encoded: tensor([[    2,  3822, 12929,  1552,  2073,   721,  2170,  2259,  1299,  1891,\n",
      "          2394,  1583,  2264,  2031,  2088,   874,  2063,  2203,  4758,  2088,\n",
      "         27116,  2181,    18,     3]])\n",
      "Tokenized: ['오늘', '##가치', '조', '##은', '날', '##에', '##는', '술', '한', '##병', '지', '##버', '##들', '##고', '뒷', '##사', '##네', '오르', '##고', '시퍼', '##라', '.']\n",
      "\n",
      "Original: 터키 에르도안 쿠르드 철수 아느면 진뭉개버릴 껃\n",
      "Encoded: tensor([[    2,  8506, 13746,  2119,  2283, 28272,  8668,  1376,  2922,  2460,\n",
      "          1585,  2693,  2019, 21992,     1,     3]])\n",
      "Tokenized: ['터키', '에르', '##도', '##안', '쿠르드', '철수', '아', '##느', '##면', '진', '##뭉', '##개', '##버릴', '[UNK]']\n",
      "\n",
      "Original: 방송·통신 기그 밀조사처넉쭝 이용자 예산 고자 기류걱…영.일퍼센트\n",
      "Encoded: tensor([[    2,  3861,   100,  4298,   645,  2029,     1,  6910,  4135,  4359,\n",
      "         15349,  3229,   121,  1437,    18,  1507,  2712,  5753,     3]])\n",
      "Tokenized: ['방송', '·', '통신', '기', '##그', '[UNK]', '이용자', '예산', '고자', '기류', '##걱', '…', '영', '.', '일', '##퍼', '##센트']\n",
      "\n",
      "Original: 靑 멕시코 1대1 비즈니스 상담회 8천600만弗 성과\n",
      "Encoded: tensor([[   2,  530, 8099,   21, 2104, 2083, 5928, 4981, 2124,   28, 2337, 9616,\n",
      "         2082, 2154,    1, 4422,    3]])\n",
      "Tokenized: ['靑', '멕시코', '1', '##대', '##1', '비즈니스', '상담', '##회', '8', '##천', '##60', '##0', '##만', '[UNK]', '성과']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 베이스코드의 설정대로 가져온 토큰화 테스트 코드\n",
    "model_name = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sentences = [\n",
    "    \"이순신은 조선 중기의 무신이다.\",\n",
    "    \"오늘가치 조은 날에는 술 한병 지버들고 뒷사네 오르고 시퍼라.\",\n",
    "    \"터키 에르도안 쿠르드 철수 아느면 진뭉개버릴 껃\",\n",
    "    \"방송·통신 기그 밀조사처넉쭝 이용자 예산 고자 기류걱…영.일퍼센트\",\n",
    "    \"靑 멕시코 1대1 비즈니스 상담회 8천600만弗 성과\"\n",
    "    ]\n",
    "\n",
    "# tokenizer.add_tokens([\"껃\"])\n",
    "encoded_sentences = [tokenizer.encode(sentence, return_tensors=\"pt\") for sentence in sentences]\n",
    "tokenized_sentences = [tokenizer.tokenize(sentence, return_tensors=\"pt\") for sentence in sentences]\n",
    "\n",
    "\n",
    "for sentence, encoded_sentence, tokenized_sentence in zip(sentences, encoded_sentences, tokenized_sentences):\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Encoded: {encoded_sentence}\")\n",
    "    print(f\"Tokenized: {tokenized_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
