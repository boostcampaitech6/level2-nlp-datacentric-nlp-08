{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.8/site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.8/site-packages (from pandas) (1.23.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: konlpy in /opt/conda/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from konlpy) (1.5.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.8/site-packages (from konlpy) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.8/site-packages (from konlpy) (1.23.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install konlpy\n",
    "!apt-get install mecab libmecab-dev mecab-ipadic-utf8\n",
    "!pip install mecab-python3\n",
    "\n",
    "!apt-get install curl git\n",
    "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "\n",
    "!apt-get install mecab-ko-dic\n",
    "\n",
    "!echo `mecab-config --dicdir`\"/mecab-ko-dic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "'''\n",
    "Cleaning_V1\n",
    "'…', '...', '·' 제거 후 strip() 적용\n",
    "'''\n",
    "file_path = \"../data/train_origin.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace '… ', '...', and '·' with ' ' and then apply strip()\n",
    "    cleaned_text = text.replace('…', ' ').replace('...', ' ').replace('..', ' ').replace('·', ' ').strip()\n",
    "    return cleaned_text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "output_file_path = '../data/train.csv'\n",
    "df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "'''\n",
    "Cleaning_V3\n",
    "모든 숫자를 0으로 치환 masking\n",
    "예) 2016 -> 0000, 64 -> 00\n",
    "'''\n",
    "file_path = \"../data/train_origin.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def clean_text_v3(text):\n",
    "    # 모든 숫자를 '0'으로 치환\n",
    "    cleaned_text = re.sub(r'\\d', '0', text)\n",
    "    return cleaned_text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text_v3)\n",
    "\n",
    "output_file_path = '../data/train_cleaning_v3.csv'\n",
    "df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "'''\n",
    "Cleaning_V4\n",
    "V2의 개선 버전\n",
    "명사, 동사만 남기고 모든 조사, 부사 등을 제거함\n",
    "기존에 붙어 있던 명사는 붙이고, 떨어져 있던 명사는 떨어트림\n",
    "'''\n",
    "def extract_nouns(sentence):\n",
    "    mecab = Mecab('/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ko-dic')\n",
    "    tokens = mecab.pos(sentence)\n",
    "\n",
    "    # Filter out unwanted tokens (keeping nouns, numbers, and English words)\n",
    "    filtered_tokens = [word for word, pos in tokens if pos.startswith('N') or pos.startswith('V') or pos == 'SN' or pos == 'SL']\n",
    "    result_tokens = []\n",
    "    \n",
    "    current_token = ''    \n",
    "    next_token = ''\n",
    "    for i in range(len(filtered_tokens) - 1):\n",
    "        current_token = filtered_tokens[i] if current_token == '' else current_token\n",
    "        next_token = filtered_tokens[i + 1]\n",
    "        \n",
    "        pattern = re.escape(current_token + next_token)\n",
    "        if re.search(pattern, sentence):\n",
    "            current_token += next_token\n",
    "        else:\n",
    "            result_tokens.append(current_token)\n",
    "            current_token = ''\n",
    "            \n",
    "    if current_token:\n",
    "        result_tokens.append(current_token)\n",
    "    else:\n",
    "        result_tokens.append(next_token)\n",
    "        \n",
    "    if len(filtered_tokens) == 1:\n",
    "        return filtered_tokens\n",
    "    \n",
    "    return result_tokens\n",
    "\n",
    "file_path = \"../data/crawling_v7.csv\"\n",
    "output_file_path = \"../data/train.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "# Create a temporary DataFrame with the same structure as df\n",
    "temp_df = df.copy()\n",
    "\n",
    "# Apply extract_nouns to all rows of temp_df['text']\n",
    "temp_df['nouns'] = temp_df['text'].apply(extract_nouns)\n",
    "\n",
    "# Join the extracted nouns with spaces in temp_df\n",
    "temp_df['joined_nouns'] = temp_df['nouns'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Overwrite the 'text' column in the original DataFrame (df) with the joined nouns\n",
    "df['text'] = temp_df['joined_nouns']\n",
    "\n",
    "df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "'''\n",
    "Cleaning_V6 with Regular Expressions\n",
    "모든 특수문자 제거\n",
    "'''\n",
    "file_path = \"../data/train_origin.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def clean_text_v6(text):\n",
    "    # 특수문자 제거\n",
    "    cleaned_text = re.sub(r'[^\\w\\sㄱ-ㅎ가-힣a-zA-Z0-9]', ' ', text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    return cleaned_text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text_v6)\n",
    "\n",
    "output_file_path = '../data/train_cleaning_v6.csv'\n",
    "df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "'''\n",
    "Cleaning_V7\n",
    "모든 [UNK] 토큰 제거\n",
    "'''\n",
    "model_name = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "file_path = \"../data/crawling.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def clean_text_v7(text):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Remove [UNK] tokens\n",
    "    tokens = [token for token in tokens if token != '[UNK]']\n",
    "    \n",
    "    # Reconstruct the cleaned text\n",
    "    cleaned_text = tokenizer.convert_tokens_to_string(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the cleaning function to the 'text' column\n",
    "df['text'] = df['text'].apply(clean_text_v7)\n",
    "\n",
    "output_file_path = '../data/crawling_v7.csv'\n",
    "df.to_csv(output_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
